# -*- coding: utf-8 -*-
"""MP_Hands_from_ONNX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yo2KgaqAd0FabZ_g_1lVW5GDlb8yt3sI


"""### Import"""

import onnx
import onnxruntime as ort
import numpy as np
from PIL import Image
from matplotlib import pyplot as plt

import math
import cv2
from math import pi
from typing import Tuple, List
import copy

"""# Palm Detection

### Helpersã…”
"""

from utils import utils

"""## Model"""

# palm_model = './hand-gesture-recognition-using-onnx/model/palm_detection/palm_detection_full_inf_post_192x192.onnx'

"""## Test Image

"""


# just shape
# x = np.zeros(shape=[1, 3, size, size], dtype=np.int8)
# x = np.zeros(shape=[1, 3, size, size], dtype=np.float32)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/hand.jpg?raw=true' -O 'test_images/hand.jpg'

# real image


"""## Post-Processing (Reverse-Engineer)"""

SCORE_THRESHOLD = 0.50
SQUARE_STANDARD_SIZE = 192
SQUARE_PADDING_HALF_SIZE = 0



# visualize_palm()
# visualize_palm(postprocess_palms(np.array(img), onnx_preds), np.array(img))

palms = res
img_ = np.array(img)
debug_image = img_.copy()

cap_width = img_.shape[1]
cap_height = img_.shape[0]
# cap_width = 224 # DEBUG
# cap_height = 224 # DEBUG
wh_ratio = 1
rects = []

if len(palms) > 0:
    for palm in palms:
        sqn_rr_size = palm[0]
        rotation = palm[1]
        sqn_rr_center_x = palm[2]
        sqn_rr_center_y = palm[3]

        cx = int(sqn_rr_center_x * cap_width)
        cy = int(sqn_rr_center_y * cap_height)
        xmin = int((sqn_rr_center_x - (sqn_rr_size / 2)) * cap_width)
        xmax = int((sqn_rr_center_x + (sqn_rr_size / 2)) * cap_width)
        ymin = int((sqn_rr_center_y - (sqn_rr_size * wh_ratio / 2)) * cap_height)
        ymax = int((sqn_rr_center_y + (sqn_rr_size * wh_ratio / 2)) * cap_height)
        xmin = max(0, xmin)
        xmax = min(cap_width, xmax)
        ymin = max(0, ymin)
        ymax = min(cap_height, ymax)
        # degree = degrees(rotation)
        # degree = math.cos(math.radians(rotation))
        degree = math.degrees(rotation)
        rects.append([cx, cy, (xmax-xmin), (ymax-ymin), degree])

    rects = np.asarray(rects, dtype=np.float32)

    # Debug
    for rect in rects:
        rects_tuple = ((rect[0], rect[1]), (rect[2], rect[3]), rect[4])
        box = cv2.boxPoints(rects_tuple).astype(np.int0)
        cv2.drawContours(debug_image, [box], 0,(0,0,255), 2, cv2.LINE_AA)

        rcx = int(rect[0])
        rcy = int(rect[1])
        half_w = int(rect[2] // 2)
        half_h = int(rect[3] // 2)
        x1 = rcx - half_w
        y1 = rcy - half_h
        x2 = rcx + half_w
        y2 = rcy + half_h
        text_x = max(x1, 10)
        text_x = min(text_x, cap_width-120)
        text_y = max(y1-15, 45)
        text_y = min(text_y, cap_height-20)
        # not_rotate_rects.append([rcx, rcy, x1, y1, x2, y2, 0])
        cv2.putText(
            debug_image,
            f'{y2-y1}x{x2-x1}',
            (text_x, text_y),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0,0,0),
            2,
            cv2.LINE_AA,
        )
        cv2.putText(
            debug_image,
            f'{y2-y1}x{x2-x1}',
            (text_x, text_y),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (59,255,255),
            1,
            cv2.LINE_AA,
        )
        cv2.rectangle(
            debug_image,
            (x1,y1),
            (x2,y2),
            (0,128,255),
            2,
            cv2.LINE_AA,
        )
        cv2.circle(
            debug_image,
            (rcx, rcy),
            3,
            (0, 255, 255),
            -1,
        )

        base_point = np.asarray(
            [rcx, rcy],
            dtype=np.float32,
        )
        # points = np.asarray(
        #     list(palm_trackid_cxcy.values()),
        #     dtype=np.float32,
        # )


# Get the palm images with corrected rotation angles
cropped_rotated_hands_images = utils.rotate_and_crop_rectangle(
    image=img_,
    rects_tmp=rects,
    operation_when_cropping_out_of_range='padding',
)

# if len(cropped_rotated_hands_images) > 0:

#     hand_landmarks, rotated_image_size_leftrights = hand_landmark(
#         images=cropped_rotated_hands_images,
#         rects=rects,
#     )

#     if len(hand_landmarks) > 0:
#         pre_processed_landmarks = []
#         pre_processed_point_histories = []
#         for (trackid, x1y1), landmark, rotated_image_size_leftright, not_rotate_rect in \
#             zip(palm_trackid_box_x1y1s.items(), hand_landmarks, rotated_image_size_leftrights, not_rotate_rects):

#             x1, y1 = x1y1
#             rotated_image_width, _, left_hand_0_or_right_hand_1 = rotated_image_size_leftright
#             thick_coef = rotated_image_width / 400
#             lines = np.asarray(
#                 [
#                     np.array([landmark[point] for point in line]).astype(np.int32) for line in lines_hand
#                 ]
#             )
#             radius = int(1+thick_coef*5)
#             cv2.polylines(
#                 debug_image,
#                 lines,
#                 False,
#                 (255, 0, 0),
#                 int(radius),
#                 cv2.LINE_AA,
#             )

viz = debug_image.copy()
plt.imshow(viz)
plt.show()

plt.subplot(1, 2, 1)
plt.title("ONNX")
plt.imshow(visualize_palm(postprocess_palms(np.array(img), onnx_preds), np.array(img)))
# plt.imshow(visualize_palm(postprocess_palms(np.array(img), onnx_preds_alt), np.array(img)))

plt.subplot(1, 2, 2)
plt.title("TVM")
plt.imshow(visualize_palm(postprocess_palms(np.array(img), tvm_outputs), np.array(img)))
plt.show()

"""## Evaluate"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # more test images
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/center.jpg?raw=true' -O 'test_images/center.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/flipped.jpg?raw=true' -O 'test_images/flipped.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/top_left.jpg?raw=true' -O 'test_images/top_left.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/top_right.jpg?raw=true' -O 'test_images/top_right.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/top_right2.jpg?raw=true' -O 'test_images/top_right2.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/bottom_left.jpg?raw=true' -O 'test_images/bottom_left.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/bottom_right.jpg?raw=true' -O 'test_images/bottom_right.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/bottom_right.jpg?raw=true' -O 'test_images/bottom_right.jpg'
# !wget 'https://github.com/k2m5t2/assets/blob/master/test_images/hand/bottom_right2.jpg?raw=true' -O 'test_images/bottom_right2.jpg'

# onnx
onnx_preds = m.run(["pdscore_boxx_boxy_boxsize_kp0x_kp0y_kp2x_kp2y"], {"input": x})[0]
print(onnx_preds.shape)
onnx_pred = onnx_preds[0]

# tvm
tvm_preds = palm_ex(tvm.nd.array(x.astype(dtype))).numpy()
print(tvm_preds.shape)
tvm_pred = tvm_preds[0]

"""### Visualize"""

imshow(visualize_palm(postprocess_palms(np.array(img), onnx_preds), np.array(img)))

imshow(visualize_palm(postprocess_palms(np.array(img), tvm_preds), np.array(img)))

"""Nice!

### Compare

Qualitative
"""

def palm_compare(fn, debug=False):
    img = Image.open("./test_images/"+fn).resize((size, size))
    x = np.array(img).transpose(2, 0, 1)[np.newaxis, ...].astype(dtype)/256

    onnx_preds = m.run(["pdscore_boxx_boxy_boxsize_kp0x_kp0y_kp2x_kp2y"], {"input": x})[0]
    onnx_pred = onnx_preds[0]
    if debug: print(onnx_pred)
    tvm_outputs = palm_ex(tvm.nd.array(x.astype(dtype))).numpy()
    tvm_output = tvm_outputs[0]
    if debug: print(tvm_output)

    plt.subplot(2, 2, 1)
    plt.title("ONNX")
    plt.axis('off')
    plt.imshow(visualize_palm(postprocess_palms(np.array(img), onnx_preds), np.array(img)))

    plt.subplot(2, 2, 2)
    plt.title("TVM")
    plt.axis('off')
    plt.imshow(visualize_palm(postprocess_palms(np.array(img), tvm_outputs), np.array(img)))

    plt.subplot(2, 2, 3)
    plt.title("ONNX - cropped")
    plt.axis('off')
    plt.imshow(viz_processed_palm(onnx_preds, img))

    plt.subplot(2, 2, 4)
    plt.title("TVM - cropped")
    plt.axis('off')
    plt.imshow(viz_processed_palm(tvm_outputs, img))

    plt.show()

palm_compare("center.jpg", debug=True)

palm_compare("flipped.jpg")

palm_compare("top_left.jpg")

palm_compare("top_right.jpg")

palm_compare("top_right2.jpg")

palm_compare("bottom_left.jpg")

palm_compare("bottom_right.jpg")

palm_compare("bottom_right2.jpg", debug=True)

"""Quantitative"""

print('Values:', onnx_pred)
print('Errors:', onnx_pred - tvm_pred)
print('Error (%):', round(np.mean(onnx_pred - tvm_pred)/np.mean(onnx_pred)*100, 5))

"""# Hand Landmark Detection

**`TODO` crop-rotate pre-processing step**. This is necessary for proper landmark detection.

## Model
"""



"""### Diagnostics

### Update Opset Version
"""

ldmk_original = onnx.load(ldmk_file)
ldmk_updated = onnx.version_converter.convert_version(ldmk_original, 15)

onnx.save(ldmk_updated, ldmk_file)

"""### NNAPI/CoreML Compatibility"""

! python -m onnxruntime.tools.check_onnx_model_mobile_usability {landmark_model}

"""## Load"""

ldmk_model = onnx.load(ldmk_file)

"""### Inspect"""

print("Input:", ldmk_model.graph.input)
print("Output:", ldmk_model.graph.output)

start_netron(ldmk_file)

"""## Test Image"""

# real image
img_path = "./test_images/hand.jpg"

"""## Build with Relay"""

x2.shape

ldmk_input_name = "input"
ldmk_shape_dict = {input_name: x2.shape}
ldmk_mod, ldmk_params = relay.frontend.from_onnx(ldmk_model, ldmk_shape_dict)

with tvm.transform.PassContext(opt_level=1):
    ldmk_ex = relay.build_module.create_executor(
        "vm", ldmk_mod, tvm.cpu(0), target, ldmk_params
    ).evaluate()

"""## Run Inference

TVM
"""

dtype = "float32"
ldmk_tvm_preds = ldmk_ex(tvm.nd.array(x2.astype(dtype)))[0].numpy()

"""ONNX"""

"""## Evaluate / Compare Results"""


"""### Quantitative"""

print('Values:', np.round(ldmk_onnx_pred, 2))
print('Errors:', np.round(ldmk_onnx_pred - ldmk_tvm_pred), 5)
print('Error (%):', round(np.mean(ldmk_onnx_pred - ldmk_tvm_pred)/np.mean(ldmk_onnx_pred)*100, 5))

# NOTE graphically compare errors
# plt.plot(ldmk_onnx_pred, label='ONNX')
# plt.plot(ldmk_tvm_pred, label='TVM')
# plt.legend()
# plt.show()

"""### Qualitative"""

def xyz_x21_to_yx(a):
    b = np.reshape(a, (21, 3))
    return b[:, 0:2]

def visualize_landmarks_0(pred, img):
    plt.imshow(img)
    ldmks = xyz_x21_to_yx(pred)
    plt.scatter(ldmks[:, 0], ldmks[:, 1], c='w', s=10)

def ldmk_compare(fn):
    # img = Image.open("./test_images/"+fn).resize((ldmk_size, ldmk_size))
    img = Image.open("./test_images/"+fn).resize((palm_size, palm_size))
    x = np.array(img).transpose(2, 0, 1)[np.newaxis, ...].astype(dtype)/256
    tvm_outputs = palm_ex(tvm.nd.array(x.astype(dtype))).numpy()
    palm = viz_processed_palm(tvm_outputs, img) # rotated, cropped palm
    palm = Image.fromarray(palm).resize((ldmk_size, ldmk_size)) # resize for ldmk model
    # x2 = np.array(img).transpose(2, 0, 1)[np.newaxis, ...].astype(dtype)/256
    x2 = np.array(palm).transpose(2, 0, 1)[np.newaxis, ...].astype(dtype)/256

    ldmk_onnx_preds = ldmk_m.run(["xyz_x21"], {"input": x2})[0]
    ldmk_onnx_pred = ldmk_onnx_preds[0]
    ldmk_tvm_preds = ldmk_ex(tvm.nd.array(x2.astype(dtype)))[0].numpy()
    ldmk_tvm_pred = ldmk_tvm_preds[0]

    plt.subplot(1, 2, 1)
    plt.title("ONNX")
    # visualize_landmarks_0(ldmk_onnx_pred, img)
    visualize_landmarks_0(ldmk_onnx_pred, palm)
    plt.subplot(1, 2, 2)
    plt.title("TVM")
    # visualize_landmarks_0(ldmk_tvm_pred, img)
    visualize_landmarks_0(ldmk_tvm_pred, palm)
    plt.show()

ldmk_compare("center.jpg")

ldmk_compare("flipped.jpg")

ldmk_compare("top_left.jpg")

ldmk_compare("top_right.jpg")

ldmk_compare("top_right2.jpg")

ldmk_compare("bottom_left.jpg")

ldmk_compare("bottom_right.jpg")

ldmk_compare("bottom_right2.jpg")

"""`TODO` perhaps a better viz function (based on hand-onnx repo?) (including lines, maybe colors)"""



"""# Optimize

Let's skip optimization for now. Must do a couple of Relax/Metaschedule examples before going ahead. (Which means, let's get TVM Guru built first!)

- Test inference @ different runtimes ()
- Benchmark
- RPC
"""

import tvm.meta_schedule as ms
from tvm import runtime, transform
from tvm.ir.module import IRModule
from typing import Dict
from tvm.target.target import Target

num_trials = 1000

work_dir = './tune_tmp'

target = 'llvm -num-cores 8'

def apply_opt_before_tuning(relay_mod: IRModule, params: Dict[str, runtime.NDArray] = None, target: Target = None):
    with transform.PassContext(opt_level=3):
        relay_mod = relay.transform.SimplifyInference()(relay_mod)
        relay_mod = relay.transform.FoldConstant()(relay_mod)
        relay_mod = relay.transform.FoldScaleAxis()(relay_mod)
        relay_mod = relay.transform.CanonicalizeOps()(relay_mod)
        relay_mod = relay.transform.AlterOpLayout()(relay_mod)
        relay_mod = relay.transform.FoldConstant()(relay_mod)

    return relay_mod

palm_mod_opt1 = apply_opt_before_tuning(palm_mod)

def tune(mod, params):
    db = ms.relay_integration.tune_relay(
        mod=mod,
        params=params,
        target=target,
        max_trials_global=num_trials*60,
        max_trials_per_task=num_trials,
        num_trials_per_iter=64,
        # runner=get_runner(),
        work_dir=work_dir,
    )
    return db

# palm_relay_db = tune(palm_mod_opt1, palm_params)

"""# Compile"""

dir(tvm.runtime)

dir(tvm.runtime.vm)

# from tvm import relax

# from tvm.relax.testing import relay_translator
# import tvm.relax.testing.relay_translator
import tvm.relay

import tvm.relay.backend

exct = tvm.relay.backend.Executor("vm")

# executor that is already created during import step

palm_ex = relay.build_module.create_executor(
        # "graph", mod, tvm.cpu(0), target, params
        "vm", palm_mod, tvm.cpu(0), target, palm_params
    ).evaluate()

# # NOTE so maybe this isn't needed at all!
ex = tvm.relay.build(ir_mod=palm_mod, target="llvm", executor=exct)

# the model involves dynamic shape, requiring use of relax
# however, relax fails to import except at latest builds of TVM
# yet the latest builds suffer from TIR.ProducerLoad error.
# all in all, best bet is to go ahead with what exists so far.

# palm_rxt_mod = relay_translator.from_relay(palm_mod["main"], target)

# palm_rxt_ex = relax.build(ir_mod=palm_rxt_mod, target="llvm")

palm_ex.export_library("./mp_hands_palm.so")

palm_ex

dir(palm_ex)

!exec ./mp_hands_palm/so

"""# Deploy

Targets:

- C++
- Wasm
- Android
- iOS
- Flutter

## C++
"""

device = tvm.cpu()

palm_dex = tvm.runtime.load_module("./mp_hands_palm.so")
palm_dvm = relay.VirtualMachine(rt_mod=palm_dex, device=device)

palm_dvm_preds = palm_dvm["main"](tvm.nd.array(x2.astype(dtype))).numpy()

palm_dvm_preds[0, 0, ...]
# plt.imshow(palm_dvm_preds[0, 0, ...])

"""# Apply

- infer, visualize in real time @ Flutter (w/ Pixel)

# Notes
"""

